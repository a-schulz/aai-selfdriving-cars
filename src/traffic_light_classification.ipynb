{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70aed2d7",
   "metadata": {},
   "source": [
    "# Traffic light classification \n",
    "This Notebook consists of 3 Parts\n",
    "\n",
    "* Set up environment \n",
    "  * load modules \n",
    "  * load paths from .env\n",
    "  * convert Yolo V8 data format to this from the lecture\n",
    "* Dataset generation\n",
    "  * Detecting traffic lights with yolo\n",
    "  * Results postprocessing\n",
    "* CNN \n",
    "  * define CNN\n",
    "  * Training \n",
    "\n",
    "\n",
    "The dataset generation works in this notebook version with a \"original\" dataset which already obtain pictures which are labeled \n",
    "with the parent folder name.\n",
    "The classes of the original dataset should be \"red\", \"green\", \"yellow\".\n",
    "\n",
    "The resulting dataset folder consists of 3 folder:\n",
    "* original_data\n",
    "  consists of images with the labels from above \n",
    "* custom_data\n",
    "  The dataset where the CNN is trained on.\n",
    "* drop_outs\n",
    "  consists of images which are to similar to other images in the same classes to make sure that data is not to much correlated\n",
    "\n",
    "\n",
    "> If the custom dataset with the classes \"green\", \"yellow\", \"red\" and \"none\" already exists, then it is possible to define and train the CNN after the \n",
    "> [CNN](#cnn) Markdown cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3603d50",
   "metadata": {},
   "source": [
    "# Set up environment\n",
    "* Load modules\n",
    "* Load paths from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02902c7b-e530-47e5-94c0-6e3f10227ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_utils as cu\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load .env files\n",
    "load_dotenv()\n",
    "\n",
    "# Get environment variables\n",
    "dataset_dir = os.getenv('DATASET_DIR')\n",
    "ori_data_dir = os.getenv('TRAFFIC_LIGHT_ORIGINAL_DATA')\n",
    "custom_data_dir = os.getenv('TRAFFIC_LIGHT_CUSTOM_DATA')\n",
    "drop_data_dir = os.getenv('TRAFFIC_LIGHT_DROP_DATA')\n",
    "\n",
    "# check if folders exists or create them\n",
    "if not os.path.exists(ori_data_dir):\n",
    "    print(\"Error: No original data set\")\n",
    "\n",
    "if not os.path.exists(custom_data_dir):\n",
    "    os.mkdir(custom_data_dir)\n",
    "\n",
    "if not os.path.exists(drop_data_dir):\n",
    "    os.mkdir(drop_data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f445bf7",
   "metadata": {},
   "source": [
    "### convert yolo data format to lecture format\n",
    "1. download dataset from [Roboflow](https://universe.roboflow.com/wawan-pradana/cinta_v2/dataset/1)\n",
    "2. extract it and copy its content to aai-selfdriving-cars/dataset/traffic_light/original_data\n",
    "3. execute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6190168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the yolo v8 data set and extract its labels \n",
    "labels = cu.convert_dataset(ori_data_dir, ori_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06acc828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label folders \n",
    "for l in labels:\n",
    "    custom_label_dir = os.path.join(custom_data_dir, l)\n",
    "    drop_label_dir = os.path.join(drop_data_dir, l)\n",
    "\n",
    "    if not os.path.exists(custom_label_dir):\n",
    "        os.mkdir(custom_label_dir)\n",
    "    if not os.path.exists(drop_label_dir):\n",
    "        os.mkdir(drop_label_dir)\n",
    "\n",
    "none_path = os.path.join(custom_data_dir, \"none\")\n",
    "if not os.path.exists(none_path):\n",
    "    os.mkdir(none_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ba111",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "### load images from original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c9e9e6-d8d4-4b7a-86cc-58fd3f0898f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dic = {}\n",
    "for l in labels: \n",
    "    paths, images = cu.get_images(os.path.join(ori_data_dir, l), n=100) # this loads  images from each class label in an array\n",
    "    # label array pair get stored in dictionary \n",
    "    img_dic.update({l: images })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f2ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot images for debug not necessary \n",
    "for c in img_dic:\n",
    "    i = 0\n",
    "    while i < len(img_dic[c]):\n",
    "        cv2.imshow(\"Hi\", img_dic[c][i])\n",
    "        key = cv2.waitKey(0)\n",
    "\n",
    "        if key == ord(\"0\"):\n",
    "            break\n",
    "        elif key == ord(\"1\"):\n",
    "            i += 1\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf09ae",
   "metadata": {},
   "source": [
    "### Detecting traffic lights with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained YOLO medium size model\n",
    "model = YOLO(\"yolov8m.pt\") # test with sample images shows that, size m is a good mid way between accuracy and run time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0337a22f",
   "metadata": {},
   "source": [
    "### analyze images and extract boxes with YOLO \n",
    "\n",
    "Images of every class are passed to yolo.\n",
    "    \n",
    "Its output get parsed in extract_boxes which identify all traffic light boxes (boxes with number 9) and return just the box as an new image.\n",
    "    \n",
    "res_dic = {\"label1\", [array of extracted boxes of folder from label1], \"label2\" : [boxes array from label2]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75ad51-17da-4cbf-9208-4b1682ce77cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "for l in labels:\n",
    "    # analyze all images from one class at the same time \n",
    "    res = model.predict(img_dic[l], conf=0.3)\n",
    "    # extract all traffic light boxes (9 is the class number for traffic lights in yolov8)\n",
    "    boxes = cu.extract_boxes(res,9)\n",
    "    res_dict.update({l : boxes})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28578e59",
   "metadata": {},
   "source": [
    "### resize the extracted images\n",
    "Because the images has different sizes it necessary to resize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e8ea2-5a0e-44b3-a58f-38075cfae35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_resize(res_dict):\n",
    "    hights = []\n",
    "    widths = []\n",
    "    for l in res_dict:\n",
    "        hights += [i.shape[0] for i in res_dict[l]]\n",
    "        widths += [i.shape[1] for i in res_dict[l]]\n",
    "    avr_x = int(np.average(widths))\n",
    "    avr_y = int(np.average(hights))\n",
    "    for l in res_dict:\n",
    "        for idx, i in enumerate(res_dict[l]):\n",
    "            res_dict[l][idx] = cv2.resize(i, (avr_x,avr_y))\n",
    "    return res_dict, (avr_x, avr_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize all images to the same average hight and width \n",
    "res_dict, target_size = average_resize(res_dict) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save resized images in custom_data\n",
    "for l in labels:\n",
    "    path = os.path.join(custom_data_dir, l)\n",
    "    cu.write_images(res_dict[l], path, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29cf57e",
   "metadata": {},
   "source": [
    "### manuel label check\n",
    "YOLO detects also traffic lights which not facing the camera. This Traffic lights cant have a class of green yellow red. \n",
    "So it is necessary to go through the dataset and set the label of all images with this properties to \"none\".\n",
    "The following function helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed03b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(cus_dic):\n",
    "    # program to iterate over all images and decide if they are in class \"none\"\n",
    "    # the other images are already in right class directory \n",
    "    for l in cus_dic:\n",
    "        curr_img = 0\n",
    "        imgs = cus_dic[l][0]\n",
    "        imgs_paths = cus_dic[l][1]\n",
    "        \n",
    "        none_img = []\n",
    "        none_paths = []\n",
    "\n",
    "        while curr_img < len(cus_dic[l][0]):\n",
    "            basename = os.path.basename(imgs_paths[curr_img])\n",
    "            cv2.imshow(\"Label: \" + l + \" \" + basename, imgs[curr_img])\n",
    "            key = cv2.waitKey(0)\n",
    "\n",
    "            if key == ord(\"0\"):\n",
    "                break\n",
    "            elif key == ord(\"w\"):\n",
    "                curr_img += 1\n",
    "            elif key == ord(\"n\"):\n",
    "                none_path = os.path.join(custom_data_dir, \"none\", basename)\n",
    "                os.rename(imgs_paths[curr_img], none_path)\n",
    "                none_img.append(imgs[curr_img])\n",
    "                none_paths.append(imgs_paths[curr_img])\n",
    "                imgs.pop(curr_img)\n",
    "                imgs_paths.pop(curr_img)\n",
    "\n",
    "                curr_img += 1\n",
    "            cv2.destroyAllWindows()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    cus_dic.update({\"none\" : [imgs, imgs_paths]})\n",
    "    return cus_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just if you want to try show_images without running all above once again\n",
    "res_dict = {}\n",
    "labels = os.listdir(custom_data_dir)\n",
    "for l in labels:\n",
    "    custom_cls_path = os.path.join(custom_data_dir, l)\n",
    "    paths , custom_images = cu.get_images(custom_cls_path)\n",
    "    res_dict.update({l : [custom_images, paths]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ae5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a581fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_similar(custom_dict=None, threshold=0.6):\n",
    "    # find similar pictures \n",
    "    for l in custom_dict:\n",
    "        #similar_images = []\n",
    "\n",
    "        images = custom_dict[l][0]\n",
    "        paths = custom_dict[l][1]\n",
    "\n",
    "        # compare each picture\n",
    "        i = 0\n",
    "        while i < len(images):\n",
    "            for j in range(i + 1, len(images)):\n",
    "                if j == len(images):\n",
    "                    break\n",
    "                # convert images into grayscale (ssim works only with that)\n",
    "                gray_image1 = cv2.cvtColor(images[i], cv2.COLOR_BGR2GRAY)\n",
    "                gray_image2 = cv2.cvtColor(images[j], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # calculate ssim value\n",
    "                similarity_score, _ = ssim(gray_image1, gray_image2, full=True)\n",
    "\n",
    "                # check if ssim is bigger than threshold to find most similar pictures\n",
    "                if similarity_score > threshold:\n",
    "\n",
    "                    # drop images[i] to other folder:\n",
    "                    pa = os.path.join(drop_data_dir, l,\n",
    "                                      os.path.basename(paths[i]))\n",
    "                    os.rename(paths[i], pa)\n",
    "                    paths.pop(i)\n",
    "                    images.pop(i)\n",
    "            i += 1\n",
    "\n",
    "        custom_dict[l][0] = images\n",
    "        custom_dict[l][1] = paths\n",
    "\n",
    "    return custom_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16beb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cus_dic = {}\n",
    "for l in labels:\n",
    "    custom_cls_path = os.path.join(custom_data_dir, l)\n",
    "    paths , custom_images = cu.get_images(custom_cls_path)\n",
    "    cus_dic.update({l : [custom_images, paths]})\n",
    "    cus_dic = drop_similar(cus_dic, threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a0828",
   "metadata": {},
   "source": [
    "## CNN\n",
    "### preprocess the data\n",
    "The following code is copied from the lecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e4246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the dataset directory\n",
    "dataset_dir = custom_data_dir\n",
    "\n",
    "# Initialize lists to store the images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over the subdirectories in the dataset directory\n",
    "for subdir in os.listdir(dataset_dir):\n",
    "    subdir_path = os.path.join(dataset_dir, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        # Extract the class label from the subdirectory name\n",
    "        label = subdir\n",
    "        # Iterate over the image files in the subdirectory\n",
    "        for file_name in os.listdir(subdir_path):\n",
    "            # Read the image file\n",
    "            image_path = os.path.join(subdir_path, file_name)\n",
    "            image = cv2.imread(image_path)\n",
    "            # Preprocess the image (resize, normalize, etc.) - already done\n",
    "            # Add the preprocessed image and label to the lists\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "images_sh = images.shape\n",
    "print(images_sh)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_images, test_images, train_labels0, test_labels0 = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors\n",
    "classes = np.unique(labels)\n",
    "num_classes = len(classes)\n",
    "print(classes)\n",
    "print(num_classes)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels1 = label_encoder.fit_transform(train_labels0)\n",
    "test_labels1  = label_encoder.fit_transform(test_labels0)\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels1, num_classes)\n",
    "test_labels = to_categorical(test_labels1, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b5f70",
   "metadata": {},
   "source": [
    "### define the CNN \n",
    "We oriented us at the VGG16 model and the Car Classification model from the lecture.\n",
    "We increased the number of convolutional layers and experiments a bit with the layers. The following model seams to be really good but they might exists much better architectures for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e622ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddcd75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "image_height = images_sh[1] \n",
    "image_width = images_sh[2]\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add more convolutional and pooling layers if desired\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "\n",
    "# Flatten the output from the previous layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7dfadc",
   "metadata": {},
   "source": [
    "### train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ff418",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=num_epochs, batch_size=batch_size, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64998784",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0b0cb",
   "metadata": {},
   "source": [
    "### save the trained model\n",
    "The model needs to be saved to be reused in the main file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model for reuse in main\n",
    "model.save('./traffic_light_model/traffic_light_model_cpu.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
