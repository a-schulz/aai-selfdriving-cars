{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02902c7b-e530-47e5-94c0-6e3f10227ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_utils as cu\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "labels = [\"green\", \"red\", \"yellow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1150d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load .env files\n",
    "load_dotenv()\n",
    "\n",
    "# Get environment variables\n",
    "dataset_dir = os.getenv('DATASET_DIR')\n",
    "ori_data_dir = os.path.join(dataset_dir, \"traffic_light\", \"original_data\")\n",
    "custom_data_dir = os.path.join(dataset_dir, \"traffic_light\", \"custom_data\")\n",
    "\n",
    "if not os.path.exists(ori_data_dir):\n",
    "    print(\"Error: No original data set\")\n",
    "\n",
    "if not os.path.exists(custom_data_dir):\n",
    "    os.mkdir(custom_data_dir)\n",
    "\n",
    "for l in labels:\n",
    "    label_dir = os.path.join(custom_data_dir, l)\n",
    "    if not os.path.exists(label_dir):\n",
    "        os.mkdir(label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c9e9e6-d8d4-4b7a-86cc-58fd3f0898f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 0: get images \n",
    "img_dic = {}\n",
    "for l in labels: \n",
    "    p, images = cu.get_images(os.path.join(ori_data_dir, l), 4)\n",
    "    img_dic.update({l: images })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f75ad51-17da-4cbf-9208-4b1682ce77cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 traffic light, 1: 640x640 2 traffic lights, 2: 640x640 1 traffic light, 3: 640x640 2 traffic lights, 399.5ms\n",
      "Speed: 4.2ms preprocess, 99.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1m/home/jay/module/ai_app/self_driving_cars/aai-selfdriving-cars/runs/detect/predict14\u001b[0m\n",
      "4 labels saved to /home/jay/module/ai_app/self_driving_cars/aai-selfdriving-cars/runs/detect/predict14/labels\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ultralytics.engine.results.Results'>\n",
      "red\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 traffic light, 1: 640x640 (no detections), 2: 640x640 2 traffic lights, 3: 640x640 2 traffic lights, 398.6ms\n",
      "Speed: 2.9ms preprocess, 99.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1m/home/jay/module/ai_app/self_driving_cars/aai-selfdriving-cars/runs/detect/predict14\u001b[0m\n",
      "4 labels saved to /home/jay/module/ai_app/self_driving_cars/aai-selfdriving-cars/runs/detect/predict14/labels\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ultralytics.engine.results.Results'>\n",
      "yellow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 3 traffic lights, 3 apples, 1: 640x640 (no detections), 2: 640x640 (no detections), 3: 640x640 1 person, 3 cars, 427.9ms\n",
      "Speed: 3.3ms preprocess, 107.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1m/home/jay/module/ai_app/self_driving_cars/aai-selfdriving-cars/runs/detect/predict14\u001b[0m\n",
      "4 labels saved to /home/jay/module/ai_app/self_driving_cars/aai-selfdriving-cars/runs/detect/predict14/labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ultralytics.engine.results.Results'>\n"
     ]
    }
   ],
   "source": [
    "# step 1: analyze images with yolo\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "res_dict = {}\n",
    "for l in labels:\n",
    "    print(l)\n",
    "    res = model.predict(img_dic[l], save=True, save_txt = True)\n",
    "    print(type(res[0]))\n",
    "    res_dict.update({l : res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb63051-f1d3-4aca-8dca-b8ad63120162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_list = [[1.0, 2.0, 3.0, 4.0], [1.2, 2.2, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [1.1, 2.1, 3.1, 4.1]]\\nthreshold = 0.2\\nfiltered_list = drop_similar_boxes(input_list, threshold)\\nprint(filtered_list)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo \n",
    "# drop similar boxes is requiered becaus some times yolo identify the same object with a different bounding box\n",
    "\"\"\"\n",
    "def drop_similar_boxes(input_list, threshold):\n",
    "    def are_elements_similar(elem1, elem2):\n",
    "        x1_diff = abs(elem1[0] - elem2[0])\n",
    "        y1_diff = abs(elem1[1] - elem2[1])\n",
    "        x2_diff = abs(elem1[2] - elem2[2])\n",
    "        y2_diff = abs(elem1[3] - elem2[3])\n",
    "        return x1_diff < threshold or y1_diff < threshold or x2_diff < threshold or y2_diff < threshold\n",
    "\n",
    "    unique_elements = []\n",
    "    for elem in input_list:\n",
    "        is_similar = False\n",
    "        for unique_elem in unique_elements:\n",
    "            if are_elements_similar(elem, unique_elem):\n",
    "                is_similar = True\n",
    "                break\n",
    "        if not is_similar:\n",
    "            unique_elements.append(elem)\n",
    "    return unique_elements\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "input_list = [[1.0, 2.0, 3.0, 4.0], [1.2, 2.2, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [1.1, 2.1, 3.1, 4.1]]\n",
    "threshold = 0.2\n",
    "filtered_list = drop_similar_boxes(input_list, threshold)\n",
    "print(filtered_list)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c9e8ea2-5a0e-44b3-a58f-38075cfae35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_resize(label, images):\n",
    "    print(\"type of images: \")\n",
    "    print(type(images[0]))\n",
    "    dia_len = [(i.shape[1]**2 + i.shape[2])**0.5 for i in images]\n",
    "    avr = int(np.average(dia_len))\n",
    "    for idx, i in enumerate(images):\n",
    "        print(\"hi\")\n",
    "        i = cv2.resize(i, (avr,avr))\n",
    "        #cv2.imwrite(os.path.join(custom_data_dir, label, str(idx), \".jpg\"), i) # not nessecary that label is in the name of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "782aff74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(res_dict[\"red\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba3da4d2-a2d4-42fa-9c28-fd35bf7d8f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "type of images: \n",
      "<class 'numpy.ndarray'>\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "/home/jay/module/ai_app/self_driving_cars/aai-selfdriving-cars/dataset/traffic_light/custom_data/green\n",
      "red\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "type of images: \n",
      "<class 'numpy.ndarray'>\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "/home/jay/module/ai_app/self_driving_cars/aai-selfdriving-cars/dataset/traffic_light/custom_data/red\n",
      "yellow\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "type of images: \n",
      "<class 'numpy.ndarray'>\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "/home/jay/module/ai_app/self_driving_cars/aai-selfdriving-cars/dataset/traffic_light/custom_data/yellow\n"
     ]
    }
   ],
   "source": [
    "# step 2: extract all traffic lights from all analyzed images and create for each an new image\n",
    "\n",
    "# Process results list\n",
    "image_name = 'traffic_light_green'\n",
    "file_type = '.jpg'\n",
    "\n",
    "counter = 0\n",
    "extracted_dic = {}\n",
    "for l in labels:\n",
    "    print(l)\n",
    "    ex_boxes = [] # is an list because probably there are more traffic_lights objects in a picture\n",
    "    for j, result in enumerate(res_dict[l]):\n",
    "        for i,c in enumerate(result.boxes.cls.numpy()):\n",
    "            # find traffic light boxes\n",
    "\n",
    "            points = result.boxes.xyxyn.numpy()[i]\n",
    "            if c == 9:\n",
    "                #path = out_path + image_name + str(counter) + file_type\n",
    "                extracted = cu.extract_rectangle_from_image(images[j], points) \n",
    "                ex_boxes.append(extracted)\n",
    "                res_dict[l][j] = extracted # extracted.append(new_image)\n",
    "                print(type(res_dict[l][j]))\n",
    "                counter += 1\n",
    "        extracted_dic.update({l : ex_boxes})\n",
    "            \n",
    "    # resize imgaes\n",
    "    average_resize(l, extracted_dic[l])\n",
    "    out_path = os.path.join(custom_data_dir, l)\n",
    "    print(out_path)\n",
    "    cu.write_images(extracted_dic[l], out_path, main_name=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbac9ac-795b-4eb2-8836-2effd1132d50",
   "metadata": {},
   "source": [
    "# step 3: check images manually and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0de74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49098ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
