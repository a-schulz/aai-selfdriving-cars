{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70aed2d7",
   "metadata": {},
   "source": [
    "# Traffic light classification \n",
    "This Notebook consists of 3 Parts\n",
    "\n",
    "* Set up environment \n",
    "  * load modules \n",
    "  * load paths from .env\n",
    "  * convert Yolo V8 data format to this from the lecture\n",
    "* Dataset generation\n",
    "  * Detecting traffic lights with yolo\n",
    "  * Results postprocessing\n",
    "* CNN \n",
    "  * define CNN\n",
    "  * Training \n",
    "\n",
    "\n",
    "The dataset generation works in this notebook version with a \"original\" dataset which already obtain pictures which are labeled \n",
    "with the parent folder name.\n",
    "The classes of the original dataset should be \"red\", \"green\", \"yellow\".\n",
    "\n",
    "The resulting dataset folder consists of 3 folder:\n",
    "* original_data\n",
    "  consists of images with the labels from above \n",
    "* custom_data\n",
    "  The dataset where the CNN is trained on.\n",
    "* drop_outs\n",
    "  consists of images which are to similar to other images in the same classes to make sure that data is not to much correlated\n",
    "\n",
    "\n",
    "> If the custom dataset with the classes \"green\", \"yellow\", \"red\" and \"none\" already exists, then it is possible to define and train the CNN after the \n",
    "> [CNN](#cnn) Markdown cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3603d50",
   "metadata": {},
   "source": [
    "# Set up environment\n",
    "* Load modules\n",
    "* Load paths from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02902c7b-e530-47e5-94c0-6e3f10227ae0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T07:20:22.401447924Z",
     "start_time": "2023-08-17T07:20:22.372393713Z"
    }
   },
   "outputs": [],
   "source": [
    "import custom_utils as cu\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1150d5b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T07:26:58.210139411Z",
     "start_time": "2023-08-17T07:26:58.187255554Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepare environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load .env files\n",
    "load_dotenv()\n",
    "\n",
    "# Get environment variables\n",
    "dataset_dir = os.getenv('DATASET_DIR')\n",
    "ori_data_dir = os.getenv('TRAFFIC_LIGHT_ORIGINAL_DATA')\n",
    "custom_data_dir = os.getenv('TRAFFIC_LIGHT_CUSTOM_DATA')\n",
    "drop_data_dir = os.getenv('TRAFFIC_LIGHT_DROP_DATA')\n",
    "\n",
    "# check if folders exists or create them\n",
    "if not os.path.exists(ori_data_dir):\n",
    "    print(\"Error: No original data set\")\n",
    "\n",
    "if not os.path.exists(custom_data_dir):\n",
    "    os.mkdir(custom_data_dir)\n",
    "\n",
    "if not os.path.exists(drop_data_dir):\n",
    "    os.mkdir(drop_data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f445bf7",
   "metadata": {},
   "source": [
    "### convert yolo data format to lecture format\n",
    "1. download dataset from [Roboflow](https://universe.roboflow.com/wawan-pradana/cinta_v2/dataset/1)\n",
    "2. extract it and copy its content to aai-selfdriving-cars/dataset/traffic_light/original_data\n",
    "3. execute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6190168f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T07:27:01.610367202Z",
     "start_time": "2023-08-17T07:27:01.582059976Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert the yolo v8 data set and extract its labels \n",
    "labels = cu.convert_dataset(ori_data_dir, ori_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06acc828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T07:27:02.233843979Z",
     "start_time": "2023-08-17T07:27:02.198380183Z"
    }
   },
   "outputs": [],
   "source": [
    "# create label folders \n",
    "for l in labels:\n",
    "    custom_label_dir = os.path.join(custom_data_dir, l)\n",
    "    drop_label_dir = os.path.join(drop_data_dir, l)\n",
    "\n",
    "    if not os.path.exists(custom_label_dir):\n",
    "        os.mkdir(custom_label_dir)\n",
    "    if not os.path.exists(drop_label_dir):\n",
    "        os.mkdir(drop_label_dir)\n",
    "\n",
    "none_path = os.path.join(custom_data_dir, \"none\")\n",
    "if not os.path.exists(none_path):\n",
    "    os.mkdir(none_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ba111",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "### load images from original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5c9e9e6-d8d4-4b7a-86cc-58fd3f0898f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T07:27:04.283721501Z",
     "start_time": "2023-08-17T07:27:04.252509480Z"
    }
   },
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/home/a-schulz/Projects/aai-selfdriving-cars/dataset/traffic_light/original_data/.keep'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotADirectoryError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m img_dic \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m labels: \n\u001B[0;32m----> 3\u001B[0m     paths, images \u001B[38;5;241m=\u001B[39m \u001B[43mcu\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mori_data_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ml\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# this loads  images from each class label in an array\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m# label array pair get stored in dictionary \u001B[39;00m\n\u001B[1;32m      5\u001B[0m     img_dic\u001B[38;5;241m.\u001B[39mupdate({l: images })\n",
      "File \u001B[0;32m~/Projects/aai-selfdriving-cars/src/custom_utils.py:23\u001B[0m, in \u001B[0;36mget_images\u001B[0;34m(dir, n, target_size)\u001B[0m\n\u001B[1;32m     21\u001B[0m images \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     22\u001B[0m paths \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 23\u001B[0m dir_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mdir\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, file \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dir_content):\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m n:\n",
      "\u001B[0;31mNotADirectoryError\u001B[0m: [Errno 20] Not a directory: '/home/a-schulz/Projects/aai-selfdriving-cars/dataset/traffic_light/original_data/.keep'"
     ]
    }
   ],
   "source": [
    "img_dic = {}\n",
    "for l in labels: \n",
    "    paths, images = cu.get_images(os.path.join(ori_data_dir, l), n=100) # this loads  images from each class label in an array\n",
    "    # label array pair get stored in dictionary \n",
    "    img_dic.update({l: images })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "982f2ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T07:27:18.894711858Z",
     "start_time": "2023-08-17T07:27:18.882705173Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot images for debug not necessary \n",
    "for c in img_dic:\n",
    "    i = 0\n",
    "    while i < len(img_dic[c]):\n",
    "        cv2.imshow(\"Hi\", img_dic[c][i])\n",
    "        key = cv2.waitKey(0)\n",
    "\n",
    "        if key == ord(\"0\"):\n",
    "            break\n",
    "        elif key == ord(\"1\"):\n",
    "            i += 1\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf09ae",
   "metadata": {},
   "source": [
    "### Detecting traffic lights with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff1f7a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-17T07:17:28.878652587Z"
    }
   },
   "outputs": [],
   "source": [
    "# load pretrained YOLO medium size model\n",
    "model = YOLO(\"yolov8m.pt\") # test with sample images shows that, size m is a good mid way between accuracy and run time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0337a22f",
   "metadata": {},
   "source": [
    "### analyze images and extract boxes with YOLO \n",
    "\n",
    "Images of every class are passed to yolo.\n",
    "    \n",
    "Its output get parsed in extract_boxes which identify all traffic light boxes (boxes with number 9) and return just the box as an new image.\n",
    "    \n",
    "res_dic = {\"label1\", [array of extracted boxes of folder from label1], \"label2\" : [boxes array from label2]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75ad51-17da-4cbf-9208-4b1682ce77cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T07:17:28.892028442Z",
     "start_time": "2023-08-17T07:17:28.884906739Z"
    }
   },
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "for l in labels:\n",
    "    # analyze all images from one class at the same time \n",
    "    res = model.predict(img_dic[l], conf=0.3)\n",
    "    # extract all traffic light boxes (9 is the class number for traffic lights in yolov8)\n",
    "    boxes = cu.extract_boxes(res,9)\n",
    "    res_dict.update({l : boxes})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28578e59",
   "metadata": {},
   "source": [
    "### resize the extracted images\n",
    "Because the images has different sizes it necessary to resize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e8ea2-5a0e-44b3-a58f-38075cfae35d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-17T07:17:28.888258573Z"
    }
   },
   "outputs": [],
   "source": [
    "def average_resize(res_dict):\n",
    "    hights = []\n",
    "    widths = []\n",
    "    for l in res_dict:\n",
    "        hights += [i.shape[0] for i in res_dict[l]]\n",
    "        widths += [i.shape[1] for i in res_dict[l]]\n",
    "    avr_x = int(np.average(widths))\n",
    "    avr_y = int(np.average(hights))\n",
    "    for l in res_dict:\n",
    "        for idx, i in enumerate(res_dict[l]):\n",
    "            res_dict[l][idx] = cv2.resize(i, (avr_x,avr_y))\n",
    "    return res_dict, (avr_x, avr_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3a594",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-17T07:17:28.891552894Z"
    }
   },
   "outputs": [],
   "source": [
    "# resize all images to the same average hight and width \n",
    "res_dict, target_size = average_resize(res_dict) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9e7b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T07:17:29.010435531Z",
     "start_time": "2023-08-17T07:17:28.895435906Z"
    }
   },
   "outputs": [],
   "source": [
    "# save resized images in custom_data\n",
    "for l in labels:\n",
    "    path = os.path.join(custom_data_dir, l)\n",
    "    cu.write_images(res_dict[l], path, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29cf57e",
   "metadata": {},
   "source": [
    "### manuel label check\n",
    "YOLO detects also traffic lights which not facing the camera. This Traffic lights cant have a class of green yellow red. \n",
    "So it is necessary to go through the dataset and set the label of all images with this properties to \"none\".\n",
    "The following function helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed03b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-17T07:17:28.900019107Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_images(cus_dic):\n",
    "    # program to iterate over all images and decide if they are in class \"none\"\n",
    "    # the other images are already in right class directory \n",
    "    for l in cus_dic:\n",
    "        curr_img = 0\n",
    "        imgs = cus_dic[l][0]\n",
    "        imgs_paths = cus_dic[l][1]\n",
    "        \n",
    "        none_img = []\n",
    "        none_paths = []\n",
    "\n",
    "        while curr_img < len(cus_dic[l][0]):\n",
    "            basename = os.path.basename(imgs_paths[curr_img])\n",
    "            cv2.imshow(\"Label: \" + l + \" \" + basename, imgs[curr_img])\n",
    "            key = cv2.waitKey(0)\n",
    "\n",
    "            if key == ord(\"0\"):\n",
    "                break\n",
    "            elif key == ord(\"w\"):\n",
    "                curr_img += 1\n",
    "            elif key == ord(\"n\"):\n",
    "                none_path = os.path.join(custom_data_dir, \"none\", basename)\n",
    "                os.rename(imgs_paths[curr_img], none_path)\n",
    "                none_img.append(imgs[curr_img])\n",
    "                none_paths.append(imgs_paths[curr_img])\n",
    "                imgs.pop(curr_img)\n",
    "                imgs_paths.pop(curr_img)\n",
    "\n",
    "                curr_img += 1\n",
    "            cv2.destroyAllWindows()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    cus_dic.update({\"none\" : [imgs, imgs_paths]})\n",
    "    return cus_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54e4c0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-17T07:17:28.904264165Z"
    }
   },
   "outputs": [],
   "source": [
    "# just if you want to try show_images without running all above once again\n",
    "res_dict = {}\n",
    "labels = os.listdir(custom_data_dir)\n",
    "for l in labels:\n",
    "    custom_cls_path = os.path.join(custom_data_dir, l)\n",
    "    paths , custom_images = cu.get_images(custom_cls_path)\n",
    "    res_dict.update({l : [custom_images, paths]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ae5d7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-17T07:17:28.945257083Z"
    }
   },
   "outputs": [],
   "source": [
    "show_images(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a581fa0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-17T07:17:28.945492602Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_similar(custom_dict=None, threshold=0.6):\n",
    "    # find similar pictures \n",
    "    for l in custom_dict:\n",
    "        #similar_images = []\n",
    "\n",
    "        images = custom_dict[l][0]\n",
    "        paths = custom_dict[l][1]\n",
    "\n",
    "        # compare each picture\n",
    "        i = 0\n",
    "        while i < len(images):\n",
    "            for j in range(i + 1, len(images)):\n",
    "                if j == len(images):\n",
    "                    break\n",
    "                # convert images into grayscale (ssim works only with that)\n",
    "                gray_image1 = cv2.cvtColor(images[i], cv2.COLOR_BGR2GRAY)\n",
    "                gray_image2 = cv2.cvtColor(images[j], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # calculate ssim value\n",
    "                similarity_score, _ = ssim(gray_image1, gray_image2, full=True)\n",
    "\n",
    "                # check if ssim is bigger than threshold to find most similar pictures\n",
    "                if similarity_score > threshold:\n",
    "\n",
    "                    # drop images[i] to other folder:\n",
    "                    pa = os.path.join(drop_data_dir, l,\n",
    "                                      os.path.basename(paths[i]))\n",
    "                    os.rename(paths[i], pa)\n",
    "                    paths.pop(i)\n",
    "                    images.pop(i)\n",
    "            i += 1\n",
    "\n",
    "        custom_dict[l][0] = images\n",
    "        custom_dict[l][1] = paths\n",
    "\n",
    "    return custom_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16beb41f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-17T07:17:28.945637658Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cus_dic = {}\n",
    "for l in labels:\n",
    "    custom_cls_path = os.path.join(custom_data_dir, l)\n",
    "    paths , custom_images = cu.get_images(custom_cls_path)\n",
    "    cus_dic.update({l : [custom_images, paths]})\n",
    "    cus_dic = drop_similar(cus_dic, threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a0828",
   "metadata": {},
   "source": [
    "## CNN\n",
    "### preprocess the data\n",
    "The following code is copied from the lecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b92e4246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T07:27:39.276329492Z",
     "start_time": "2023-08-17T07:27:39.143508925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396, 147, 74, 3)\n",
      "(396,)\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the dataset directory\n",
    "dataset_dir = custom_data_dir\n",
    "\n",
    "# Initialize lists to store the images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over the subdirectories in the dataset directory\n",
    "for subdir in os.listdir(dataset_dir):\n",
    "    subdir_path = os.path.join(dataset_dir, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        # Extract the class label from the subdirectory name\n",
    "        label = subdir\n",
    "        # Iterate over the image files in the subdirectory\n",
    "        for file_name in os.listdir(subdir_path):\n",
    "            # Read the image file\n",
    "            image_path = os.path.join(subdir_path, file_name)\n",
    "            image = cv2.imread(image_path)\n",
    "            # Preprocess the image (resize, normalize, etc.) - already done\n",
    "            # Add the preprocessed image and label to the lists\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "images_sh = images.shape\n",
    "print(images_sh)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bec6518e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T07:27:44.322392882Z",
     "start_time": "2023-08-17T07:27:40.215331152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['green' 'none' 'red' 'yellow']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_images, test_images, train_labels0, test_labels0 = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors\n",
    "classes = np.unique(labels)\n",
    "num_classes = len(classes)\n",
    "print(classes)\n",
    "print(num_classes)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels1 = label_encoder.fit_transform(train_labels0)\n",
    "test_labels1  = label_encoder.fit_transform(test_labels0)\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels1, num_classes)\n",
    "test_labels = to_categorical(test_labels1, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b5f70",
   "metadata": {},
   "source": [
    "### define the CNN \n",
    "We oriented us at the VGG16 model and the Car Classification model from the lecture.\n",
    "We increased the number of convolutional layers and experiments a bit with the layers. The following model seams to be really good but they might exists much better architectures for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e622ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T07:27:47.585314711Z",
     "start_time": "2023-08-17T07:27:47.560676Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cddcd75a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T07:28:39.340795928Z",
     "start_time": "2023-08-17T07:28:39.152543011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316, 147, 74, 3)\n",
      "(316, 4)\n",
      "(80, 147, 74, 3)\n",
      "(80, 4)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "image_height = images_sh[1] \n",
    "image_width = images_sh[2]\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add more convolutional and pooling layers if desired\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
    "\n",
    "# Flatten the output from the previous layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7dfadc",
   "metadata": {},
   "source": [
    "### train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "340ff418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T07:28:43.675077808Z",
     "start_time": "2023-08-17T07:28:41.048622716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 09:28:43.540416: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:437] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2023-08-17 09:28:43.540493: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:441] Memory usage: 11075584 bytes free, 2090336256 bytes total.\n",
      "2023-08-17 09:28:43.540551: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:451] Possibly insufficient driver version: 535.54.3\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_3/conv2d_24/Relu' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_58485/2003528281.py\", line 4, in <module>\n      model.fit(train_images, train_labels, epochs=num_epochs, batch_size=batch_size, validation_data=(test_images, test_labels))\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/sequential.py\", line 405, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\", line 321, in call\n      return self.activation(outputs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/activations.py\", line 321, in relu\n      return backend.relu(\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/backend.py\", line 5397, in relu\n      x = tf.nn.relu(x)\nNode: 'sequential_3/conv2d_24/Relu'\nDNN library is not found.\n\t [[{{node sequential_3/conv2d_24/Relu}}]] [Op:__inference_train_function_5287]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnimplementedError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m\n\u001B[1;32m      2\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtest_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[1;32m     54\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mUnimplementedError\u001B[0m: Graph execution error:\n\nDetected at node 'sequential_3/conv2d_24/Relu' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_58485/2003528281.py\", line 4, in <module>\n      model.fit(train_images, train_labels, epochs=num_epochs, batch_size=batch_size, validation_data=(test_images, test_labels))\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/sequential.py\", line 405, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\", line 321, in call\n      return self.activation(outputs)\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/activations.py\", line 321, in relu\n      return backend.relu(\n    File \"/home/a-schulz/Projects/aai-selfdriving-cars/src/.venv/lib/python3.11/site-packages/keras/src/backend.py\", line 5397, in relu\n      x = tf.nn.relu(x)\nNode: 'sequential_3/conv2d_24/Relu'\nDNN library is not found.\n\t [[{{node sequential_3/conv2d_24/Relu}}]] [Op:__inference_train_function_5287]"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=num_epochs, batch_size=batch_size, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64998784",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-17T07:17:28.946601571Z"
    }
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0b0cb",
   "metadata": {},
   "source": [
    "### save the trained model\n",
    "The model needs to be saved to be reused in the main file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b277c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-17T07:17:28.946741690Z"
    }
   },
   "outputs": [],
   "source": [
    "# save model for reuse in main\n",
    "model.save('./traffic_light_model/traffic_light_model_cpu.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
