{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02902c7b-e530-47e5-94c0-6e3f10227ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_utils as cu\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "labels = [\"green\", \"red\", \"yellow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load .env files\n",
    "load_dotenv()\n",
    "\n",
    "# Get environment variables\n",
    "dataset_dir = os.getenv('DATASET_DIR')\n",
    "ori_data_dir = os.getenv('TRAFFIC_LIGHT_ORIGINAL_DATA')\n",
    "custom_data_dir = os.getenv('TRAFFIC_LIGHT_CUSTOM_DATA')\n",
    "\n",
    "if not os.path.exists(ori_data_dir):\n",
    "    print(\"Error: No original data set\")\n",
    "\n",
    "if not os.path.exists(custom_data_dir):\n",
    "    os.mkdir(custom_data_dir)\n",
    "\n",
    "for l in labels:\n",
    "    label_dir = os.path.join(custom_data_dir, l)\n",
    "    if not os.path.exists(label_dir):\n",
    "        os.mkdir(label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c9e9e6-d8d4-4b7a-86cc-58fd3f0898f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 0: get images \n",
    "img_dic = {}\n",
    "for l in labels: \n",
    "    paths, images = cu.get_images(os.path.join(ori_data_dir, l), 2)\n",
    "    print(paths)\n",
    "    print(os.path.join(ori_data_dir, l))\n",
    "    img_dic.update({l: images })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d29deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_boxes(yolo_res, cls):\n",
    "    ex_boxes = []\n",
    "    for res in yolo_res:\n",
    "        for i, c in enumerate(res.boxes.cls.numpy()):\n",
    "            if c == cls:\n",
    "                points = res.boxes.xyxyn.numpy()[i]\n",
    "                ex_box = cu.extract_rectangle_from_image(res.orig_img, points) \n",
    "                ex_boxes.append(ex_box)\n",
    "    return ex_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e8ea2-5a0e-44b3-a58f-38075cfae35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_resize(res_dict):\n",
    "    #dia_len = [(i.shape[1]**2 + i.shape[2])**0.5 for i in images]\n",
    "    hights = []\n",
    "    widths = []\n",
    "    for l in res_dict:\n",
    "        hights.append([i.shape[0] for i in res_dict[l]])\n",
    "        widths.append([i.shape[1] for i in res_dict[l]])\n",
    "    #height, width = image.shape[:2]\n",
    "    avr_x = int(np.average(widths))\n",
    "    avr_y = int(np.average(hights))\n",
    "    for l in res_dict:\n",
    "        for idx, i in enumerate(res_dict[l]):\n",
    "            res_dict[l][idx] = cv2.resize(i, (avr_x,avr_y))\n",
    "        #cv2.imwrite(os.path.join(custom_data_dir, label, str(idx), \".jpg\"), i) # not nessecary that label is in the name of image\n",
    "    return res_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8m.pt\") # test with sample images shows that, m is a good mid way between accuracy and run time\n",
    "res_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75ad51-17da-4cbf-9208-4b1682ce77cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: analyze images and extract boxes with yolo\n",
    "for l in labels:\n",
    "    res = model.predict(img_dic[l], conf=0.3)\n",
    "    boxes = extract_boxes(res,9)\n",
    "    res_dict.update({l : boxes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize all images to the same average hight and width \n",
    "res_dict = average_resize(res_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save resized images in custom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(res_dict[\"red\"]))\n",
    "#i = res_dict[\"yellow\"][0]\n",
    "#plt.imshow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb63051-f1d3-4aca-8dca-b8ad63120162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo \n",
    "# drop similar boxes is requiered becaus some times yolo identify the same object with a different bounding box\n",
    "\"\"\"\n",
    "def drop_similar_boxes(input_list, threshold):\n",
    "    def are_elements_similar(elem1, elem2):\n",
    "        x1_diff = abs(elem1[0] - elem2[0])\n",
    "        y1_diff = abs(elem1[1] - elem2[1])\n",
    "        x2_diff = abs(elem1[2] - elem2[2])\n",
    "        y2_diff = abs(elem1[3] - elem2[3])\n",
    "        return x1_diff < threshold or y1_diff < threshold or x2_diff < threshold or y2_diff < threshold\n",
    "\n",
    "    unique_elements = []\n",
    "    for elem in input_list:\n",
    "        is_similar = False\n",
    "        for unique_elem in unique_elements:\n",
    "            if are_elements_similar(elem, unique_elem):\n",
    "                is_similar = True\n",
    "                break\n",
    "        if not is_similar:\n",
    "            unique_elements.append(elem)\n",
    "    return unique_elements\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "input_list = [[1.0, 2.0, 3.0, 4.0], [1.2, 2.2, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [1.1, 2.1, 3.1, 4.1]]\n",
    "threshold = 0.2\n",
    "filtered_list = drop_similar_boxes(input_list, threshold)\n",
    "print(filtered_list)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3da4d2-a2d4-42fa-9c28-fd35bf7d8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: extract all traffic lights from all analyzed images and create for each an new image\n",
    "\n",
    "# Process results list\n",
    "image_name = 'traffic_light_green'\n",
    "file_type = '.jpg'\n",
    "\n",
    "counter = 0\n",
    "extracted_dic = {}\n",
    "for l in labels:\n",
    "    print(l)\n",
    "    ex_images = [] # is an list because probably there are more traffic_lights objects in a picture\n",
    "    for j, result in enumerate(res_dict[l]):\n",
    "        for i,c in enumerate(result.boxes.cls.numpy()):\n",
    "            # find traffic light boxes\n",
    "\n",
    "            if c == 9:\n",
    "                points = result.boxes.xyxyn.numpy()[i]\n",
    "                extracted_image = cu.extract_rectangle_from_image(images[j], points) \n",
    "                ex_images.append(extracted_image)\n",
    "                print(extracted_image)\n",
    "                # res_dict[l][j] = extracted #\n",
    "                # print(type(res_dict[l][j]))\n",
    "                counter += 1\n",
    "    \n",
    "    extracted_dic.update({l : ex_images})\n",
    "\n",
    "\"\"\"\n",
    "# resize imgaes\n",
    "print(extracted_dic[l])\n",
    "average_resize(l, extracted_dic[l])\n",
    "out_path = os.path.join(custom_data_dir, l)\n",
    "print(out_path)\n",
    "cu.write_images(extracted_dic[l], out_path, main_name=l)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbac9ac-795b-4eb2-8836-2effd1132d50",
   "metadata": {},
   "source": [
    "# step 3: check images manually and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0de74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d72a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e4246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in res_dict:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49098ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "load_dotenv()\n",
    "p = os.getenv('TRAFFIC_LIGHT_ORIGINAL_DATA')\n",
    "p = os.path.join(p, \"green\")\n",
    "paths, imgs = cu.get_images(p,3)\n",
    "i = cv2.cvtColor(imgs[0], cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
